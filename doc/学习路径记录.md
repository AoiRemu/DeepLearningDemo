# 模型
## 弱智吧问答模型
### SEQ2SEQ 
1. 通过copilot,kimi等AI生成的代码进行训练
效果很差，AI给出的代码基本不能直接使用，有各种报错。
2. huggingface上的博客代码
效果不错，代码直接可以使用，且训练出的AI有明显提高：https://huggingface.co/blog/warm-starting-encoder-decoder

## COSPLAY聊天模型
### Causal
因果语言模型才是用于聊天的模型
huggingface学习：https://huggingface.co/learn/nlp-course/zh-CN/chapter7/6
1. CausalLM
模型太大，需要显卡才能跑
2. Lingzhi-AI
有小型的模型，但使用CPU跑还是太慢了，0.5B版本的跑一次大概十分钟
3. zxbsmk/NSFW_13B_sft
无限制的模型，可以续写白洁，估计也能用来虚拟COS
https://huggingface.co/zxbsmk/NSFW_13B_sft
他们有社区，电报：https://t.me/+JbovpBG6-gBiNDI1

# 训练记录
## 2024-09-11
- 弱智吧模型，用最初始的版本，使用爬虫的数据集多训几轮试试
## 2024-09-13
- RWKV模型使用：安装好环境跑是能跑，但加载模型的时候就报错，问题在于pytorch版本有冲突，得等下一批流量好了再重新配置环境试试
- 弱智吧模型训练：好像是用上了GPU，但速度没想象中那么快，训练完也得10分钟。爬虫的数据集有问题，里面有30多条“我是智障”的回答，所以导致模型动不动就说我是智障。将这部分脏数据删除后训练看看。
- **★**llama-3-8B：光是运行就特别慢，也不知道问题在哪，得好好看看
- 电报大佬说我这个显卡就这么跑跑不起来，得用llama.cpp
## 2024-09-14
- 查询了llama.cpp的相关东西：
### llama.cpp
#### 前期工作
1. 需要克隆仓库到本地
2. 需要c++的编译器，推荐cmake
3. 可以运行不止llama系列的模型，Qwen之类的也行，需要量化转换

### ollama
感觉是以llama为基础的一个工具，作用和llama差不多

### lingzhi-2.7B-chat
- huggingface的缓存文件可以通过先加载模型，再直接保存到另一个文件夹的方法来转换
- 远程电脑文件传输的模型文件到本机上会报错找不到config.json文件，原因是在snapshots文件夹中的文件都是软连接文件 **.symlink**,文件传输的时候并没有把软连接的连接属性复制过来。查看软连接目标的Powershell命令：
```
ChildItem -Path . -Recurse -Force | Where-Object { $_.LinkType -eq "SymbolicLink" } | Select-Object Name, Target
```
- 创建软连接的cmd命令：
```
mklink <软连接文件名> <目标路径（可以是相对路径）>
```
- 模型出现一直生成文本不停止的现象时，通常跟结束符没有正确设置有关。需要在generation_config.json文件中正确的设置 **bos_token_id**(开始字符) 和 **eos_token_id**（停止字符）。解析字符的代码：
```
tokenizer = AutoTokenizer.from_pretrained(lingzhi_model_path)
result = tokenizer.convert_ids_to_tokens(151643)
token = tokenizer.convert_tokens_to_ids("<|im_start|>")
```
lingzhi模型的generation_config.json:
```
{
  "_from_model_config": true,
  "bos_token_id": 151644,
  "eos_token_id": 151645,
  "transformers_version": "4.44.0"
}
```